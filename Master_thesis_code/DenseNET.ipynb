{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-03 15:58:53.826523: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-03 15:58:53.826558: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow \n",
    "import h5py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "import random\n",
    "import cv2\n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D,Convolution2D,BatchNormalization\n",
    "from tensorflow.keras.layers import Flatten,MaxPooling2D,Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tensorflow.keras.layers import Flatten,MaxPooling2D,Dropout, concatenate, Input\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,img_to_array\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelcamera=DenseNet121(weights='imagenet',include_top=False, input_shape=(32, 32, 3)) \n",
    "\n",
    "x1=modelcamera.output\n",
    "\n",
    "x1= GlobalAveragePooling2D()(x1)\n",
    "x1= BatchNormalization()(x1)\n",
    "x1= Dropout(0.5)(x1)\n",
    "x1= Dense(1024,activation='relu')(x1) \n",
    "x1= Dense(512,activation='relu')(x1) \n",
    "x1= BatchNormalization()(x1)\n",
    "x1= Dropout(0.5)(x1)\n",
    "\n",
    "# preds=Dense(8, activation='softmax')(x1) #FC-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelthermal=DenseNet121(weights='imagenet',include_top=False, input_shape=(32, 32, 3)) \n",
    "for i, layer in enumerate(modelthermal.layers):\n",
    "    layer._name = 'layer_' + str(i)\n",
    "\n",
    "x2=modelthermal.output\n",
    "\n",
    "x2= GlobalAveragePooling2D()(x2)\n",
    "x2= BatchNormalization()(x2)\n",
    "x2= Dropout(0.5)(x2)\n",
    "x2= Dense(1024,activation='relu')(x2) \n",
    "x2= Dense(128,activation='relu')(x2) \n",
    "x2= Dense(16,activation='relu')(x2) \n",
    "x2= BatchNormalization()(x2)\n",
    "x2= Dropout(0.5)(x2)\n",
    "\n",
    "# preds=Dense(8, activation='softmax')(x2) #FC-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat1 = concatenate([x1, x2])\n",
    "Out  = Dense(8, activation='softmax')(concat1)\n",
    "model_concat = Model(inputs=[modelcamera.inputs, modelthermal.inputs], outputs = Out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=Model(inputs=model_d.input,outputs=preds)\n",
    "model_concat.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [modelcamera, modelthermal]:\n",
    "    for layer in model.layers[:-8]:\n",
    "        layer.trainable=False\n",
    "\n",
    "    for layer in model.layers[-8:]:\n",
    "        layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_concat.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model_concat.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow.keras.utils.plot_model(model_concat, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# First Thermal model \n",
    "\n",
    "image_list = []\n",
    "\n",
    "def my_load_Train_data_1() :\n",
    "    image_list = []\n",
    "    for filename in glob.glob('/root/mandeep/Data_Augmentation/Thermal Images/training/*.jpg'): #assuming gif\n",
    "        im=Image.open(filename)\n",
    "        img = im.resize((32,32),Image.ANTIALIAS)\n",
    "        img= tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img= img.reshape(32, 32, 3)\n",
    "        img =img.astype('float32')\n",
    "        img = img / 255.0\n",
    "        image_list.append(img)\n",
    "\n",
    "    return image_list\n",
    "\n",
    "def my_load_Test_data_1() :\n",
    "    image_list = []\n",
    "    for filename in glob.glob('/root/mandeep/Data_Augmentation/Thermal Images/testing/*.jpg'): #assuming gif\n",
    "        im=Image.open(filename)\n",
    "        img = im.resize((32,32),Image.ANTIALIAS)\n",
    "        img= tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img= img.reshape(32, 32, 3)\n",
    "        img =img.astype('float32')\n",
    "        img = img / 255.0\n",
    "        image_list.append(img)\n",
    "\n",
    "    return image_list\n",
    "\n",
    "X1_train = my_load_Train_data_1()\n",
    "X1_test = my_load_Test_data_1()\n",
    "#print(len(X1_test))\n",
    "y1_train = pd.read_csv('/root/mandeep/Data_Augmentation/y_labels_training.csv')\n",
    "y1_test = pd.read_csv('/root/mandeep/Data_Augmentation/y_labels_testing.csv')\n",
    "y1_test.values.reshape(-1,1)\n",
    "\n",
    "n_classes = 8\n",
    "X1_train = np.array(X1_train)\n",
    "Y1_train = np.array(y1_train)\n",
    "X1_test = np.array(X1_test)\n",
    "Y1_test = np.array(y1_test)\n",
    "\n",
    "Y1_train = np_utils.to_categorical(Y1_train, n_classes)\n",
    "Y1_test = np_utils.to_categorical(Y1_test, n_classes)\n",
    "# len(X1_train)\n",
    "# print(Y1_test)\n",
    "X1_train.shape\n",
    "Y1_train.shape\n",
    "\n",
    "#Secound Camera model\n",
    "\n",
    "image_list = []\n",
    "\n",
    "def my_load_Train_data_2() :\n",
    "    image_list = []\n",
    "    for filename in glob.glob('/root/mandeep/Data_Augmentation/Camera Images/training/*.jpg'): #assuming gif\n",
    "        im=Image.open(filename)\n",
    "        img = im.resize((32,32),Image.ANTIALIAS)\n",
    "        img= tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img= img.reshape(32, 32, 3)\n",
    "        img =img.astype('float32')\n",
    "        img = img / 255.0\n",
    "        image_list.append(img)\n",
    "\n",
    "    return image_list\n",
    "\n",
    "def my_load_Test_data_2() :\n",
    "    image_list = []\n",
    "    for filename in glob.glob('/root/mandeep/Data_Augmentation/Camera Images/testing/*.jpg'): #assuming gif\n",
    "        im=Image.open(filename)\n",
    "        img = im.resize((32,32),Image.ANTIALIAS)\n",
    "        img= tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img= img.reshape(32, 32, 3)\n",
    "        img =img.astype('float32')\n",
    "        img = img / 255.0\n",
    "        image_list.append(img)\n",
    "\n",
    "    return image_list\n",
    "\n",
    "X2_train = my_load_Train_data_2()\n",
    "X2_test = my_load_Test_data_2()\n",
    "print(len(X2_test))\n",
    "y2_train = pd.read_csv('/root/mandeep/Data_Augmentation/y_labels_training.csv')\n",
    "y2_test = pd.read_csv('/root/mandeep/Data_Augmentation/y_labels_testing.csv')\n",
    "y2_test.values.reshape(-1,1)\n",
    "\n",
    "n_classes = 8\n",
    "X2_train = np.array(X2_train)\n",
    "Y2_train = np.array(y2_train)\n",
    "X2_test = np.array(X2_test)\n",
    "Y2_test = np.array(y2_test)\n",
    "\n",
    "Y2_train = np_utils.to_categorical(Y2_train, n_classes)\n",
    "Y2_test = np_utils.to_categorical(Y2_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the sequential model\n",
    "model_concat.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history= model_concat.fit([X1_train,X2_train],Y2_train, batch_size=128,epochs=10,validation_data=([X1_test,X2_test],Y2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(eval_loss, eval_accuracy) = model_concat.evaluate([X1_test,X2_test],Y2_test, batch_size=128, verbose=1)\n",
    "print('[INFO] Accuracy: {:.2f}%'.format(eval_accuracy * 100)) \n",
    "print('[INFO] Loss: {}'.format(eval_loss)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
