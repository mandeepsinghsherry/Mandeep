{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "5\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 50000, 50000, 10000, 10000, 19\n  y sizes: 24\nPlease provide data which shares the same first dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-0b72943132ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;31m# training the model for 10 epochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX1_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX2_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY2_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX1_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX2_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY2_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1061\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[0;32m   1064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1115\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1117\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m             label, \", \".join(str(i.shape[0]) for i in nest.flatten(data)))\n\u001b[0;32m    281\u001b[0m       \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"Please provide data which shares the same first dimension.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 50000, 50000, 10000, 10000, 19\n  y sizes: 24\nPlease provide data which shares the same first dimension."
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten, Input\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Add\n",
    "from keras.layers import concatenate\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# First Thermal model training\n",
    "\n",
    "image_list = []\n",
    "\n",
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "    # load dataset\n",
    "    (X1_train, Y1_train), (X1_test, Y1_test) = cifar10.load_data()\n",
    "    # one hot encode target values\n",
    "    Y1_train = to_categorical(Y1_train)\n",
    "    Y1_test = to_categorical(Y1_test)\n",
    "    return X1_train, X1_train, X1_test, Y1_test\n",
    "\n",
    "\n",
    "X1_train = load_dataset()\n",
    "X1_test = load_dataset()\n",
    "print(len(X1_test))\n",
    "len(X1_train)\n",
    "print(Y1_test)\n",
    "\n",
    "\n",
    "#secound model training\n",
    "\n",
    "image_list = []\n",
    "\n",
    "def my_load_Train_data_2() :\n",
    "    image_list = []\n",
    "    for filename in glob.glob('C:/Users/MANDEEP SINGH SHERRY/Desktop/bird/*.jpg'): #assuming gif\n",
    "        im=Image.open(filename)\n",
    "        img = im.resize((32,32),Image.ANTIALIAS)\n",
    "        img= tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img= img.reshape(32, 32, 3)\n",
    "        img =img.astype('float32')\n",
    "        img = img / 255.0\n",
    "        image_list.append(img)\n",
    "\n",
    "    return image_list\n",
    "\n",
    "def my_load_Test_data_2() :\n",
    "    image_list = []\n",
    "    for filename in glob.glob('C:/Users/MANDEEP SINGH SHERRY/Desktop/bird test/*.jpg'): #assuming gif\n",
    "        im=Image.open(filename)\n",
    "        img = im.resize((32,32),Image.ANTIALIAS)\n",
    "        img= tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img= img.reshape(32, 32, 3)\n",
    "        img =img.astype('float32')\n",
    "        img = img / 255.0\n",
    "        image_list.append(img)\n",
    "\n",
    "    return image_list\n",
    "\n",
    "X2_train = my_load_Train_data_2()\n",
    "X2_test = my_load_Test_data_2()\n",
    "print(len(X2_test))\n",
    "y2_train = pd.read_csv('C:/Users/MANDEEP SINGH SHERRY/Desktop/y_labels.csv')\n",
    "y2_test = [[1],\n",
    "           [1],\n",
    "           [1],\n",
    "           [0],\n",
    "           [0],\n",
    "           [0]]\n",
    "n_classes = 2\n",
    "X2_train = np.array(X2_train)\n",
    "Y2_train = np.array(y2_train)\n",
    "X2_test = np.array(X2_test)\n",
    "Y2_test = np.array(y2_test)\n",
    "\n",
    "Y2_train = np_utils.to_categorical(Y2_train, n_classes)\n",
    "Y2_test = np_utils.to_categorical(Y2_test, n_classes)\n",
    "len(X2_train)\n",
    "print(Y2_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Model 1(Thermal)\n",
    "inp1 = Input(shape=(32, 32, 3))\n",
    "\n",
    "# convolutional layer\n",
    "conv1=Conv2D(100, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu')(inp1)\n",
    "\n",
    "# convolutional layer\n",
    "conv2 =Conv2D(75, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu')(conv1)\n",
    "pool1=MaxPool2D(pool_size=(2,2))(conv2)\n",
    "conv3=Conv2D(50, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu')(pool1)\n",
    "pool2=MaxPool2D(pool_size=(2,2))(conv3)\n",
    "\n",
    "# flatten output of conv\n",
    "flat1=Flatten()(pool2)\n",
    "\n",
    "# hidden layer\n",
    "hidd1=(Dense(25, activation='relu'))(flat1)\n",
    "dense1=(Dense(10, activation='relu'))(hidd1)\n",
    "output1=Flatten()(dense1)\n",
    "#output= (Dense(10, activation='relu'))(model_1)\n",
    "\n",
    "# Model 2\n",
    "\n",
    "\n",
    "\n",
    "# convolutional layer\n",
    "inp2 = Input(shape=(32, 32, 3))\n",
    "\n",
    "# convolutional layer\n",
    "conv1 =Conv2D(100, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu')(inp2)\n",
    "\n",
    "# convolutional layer\n",
    "conv2 =Conv2D(75, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu')(conv1)\n",
    "pool1=MaxPool2D(pool_size=(2,2))(conv2)\n",
    "conv3=Conv2D(50, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu')(pool1)\n",
    "pool2=MaxPool2D(pool_size=(2,2))(conv3)\n",
    "\n",
    "# flatten output of conv\n",
    "flat1=Flatten()(pool2)\n",
    "\n",
    "# hidden layer\n",
    "hidd1=Dense(25, activation='relu')(flat1)\n",
    "dense2=Dense(10, activation='relu')(hidd1)\n",
    "output2=Flatten()(dense2)\n",
    "# concatinate two models \n",
    "\n",
    "\n",
    "concat1 = concatenate([output1, output2])\n",
    "Out  = Dense(2, activation='softmax')(concat1)\n",
    "\n",
    "model = Model(inputs=[inp1, inp2], outputs = Out)\n",
    "\n",
    "# compiling the sequential model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam') \n",
    "\n",
    "\n",
    "## output layer\n",
    "#model_1.add(Dense(2, activation='softmax'))\n",
    "\n",
    "## compiling the sequential model\n",
    "#model_1.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# training the model for 10 epochs\n",
    "model.fit([X1_train,X2_train],Y2_train, batch_size=128,epochs=4,validation_data=([X1_test,X2_test],Y2_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\MANDEEP SINGH SHERRY\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\MANDEEP SINGH SHERRY\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: concatModel\\assets\n",
      "[[0.46525982 0.5347402 ]]\n"
     ]
    }
   ],
   "source": [
    "model.save(\"concatModel\")\n",
    "# make a prediction for a new image.\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "# load the image thermal\n",
    "img1 = load_img('a4.jpg', target_size=(32, 32))\n",
    "# convert to array\n",
    "img1 = img_to_array(img1)\n",
    "# reshape into a single sample with 3 channels\n",
    "img1 = img1.reshape(1,32, 32, 3)\n",
    "# prepare pixel data\n",
    "img1 = img1.astype('float32')\n",
    "img1 = img1 / 255.0\n",
    "# for second\n",
    "img2 = load_img('apple4.jpg', target_size=(32, 32))\n",
    "# convert to array\n",
    "img2 = img_to_array(img2)\n",
    "# reshape into a single sample with 3 channels\n",
    "img2 = img2.reshape(1,32, 32, 3)\n",
    "# prepare pixel data\n",
    "img2 = img2.astype('float32')\n",
    "img2 = img2 / 255.0\n",
    "result = model.predict([img1,img2])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probality of Banana\n",
      "0.46525982\n",
      "probality of Apple\n",
      "0.5347402\n"
     ]
    }
   ],
   "source": [
    "print(\"probality of Banana\")\n",
    "print(result[0][0])\n",
    "print(\"probality of Apple\")\n",
    "print(result[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model(\"concatModel\")\n",
    "result = loaded_model.predict([img1,img2])\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
